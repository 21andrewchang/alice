hi everybody my name is Hirini and i'm
gonna be talking about how to use neural
networks to model sequences in the
previous lecture you saw how you could
use a neural network to model a data set
of many examples the difference with
sequences is that each example consists
of multiple data points there can be a
variable number of these data points per
example and the data points can depend
on each other in complicated ways so a
sequence could be something like a
sentence like this morning I took the
dog for a walk this is one example but
it consists of multiple words and the
words depend on each other another
example would be something like a
medical record one medical record would
be one example but it consists of many
measurements another example would be
something like a speech waveform where
this one waveform is an example but
again it consists of many many
measurements you've probably encountered
sequence modeling tasks in your everyday
life especially if you've used things
like Google Translate alexa or siri
tasks like machine translation and
question answering are all sequence
modeling tasks and the state of the art
in these tasks is mostly deep-learning
based
another interesting example I saw
recently was the self-parking car by
Audi when you think about it parking is
also a sequence modeling task because
parking is just a sequence of movements
and the next movement depends on all the
previous movements you can watch the
rest of this video online ok so a
sequence modeling problem now I'm just
gonna walk through a sequence modeling
problem to kind of motivate why we need
a different framework for specifically
for modeling sequences and what we
should be looking for in that framework
so the problem is predicting the next
word given these words we want to
predict what comes next
the first problem we run into is that
machine learning models that are not
explicitly designed to deal with
sequences take as input a fixed length
vector think back to the feed-forward
neural network from the first lecture
that Alexander introduced we have to
specify the size of the input
right at the outset we can't sometimes
feed in a vector of length ten other
times feed an elector a vector of length
20 it has to be fixed length so this is
kind of an issue with sequences because
sometimes we might have seen ten words
and we want to predict the next word
sometimes we might have seen four words
and we want to predict the next word so
we have to get that variable length
input into a fixed length vector one
simple way to do this would be to just
cut off the vector so say okay we're
gonna just take a fixed window force
this vector to be fixed length by only
considering the previous two words no
matter where we're making the prediction
we'll just take the previous two words
and then try to predict the next word
now we can represent these two words as
a fixed length vector by creating a
larger vector and then allocating space
in it for the first word and for the
second word we have a fixed length
vector now no matter what two words
we're using and we can feed this into a
machine learning model like a
feed-forward neural network or a
logistic regression or any other model
and try to make a prediction one thing
you might be noticing here is that by
using this fixed window we're giving
ourselves a very limited history we're
trying to predict the word walk having
only seen the words four and a this is
almost impossible but differently it's
really hard to model long term
dependencies to see this clearly
consider the word in sorry consider the
sentence in France I had a great time
and I learned some of the blank language
where we're trying to predict the word
in the blank I knew it was French but
that's because I looked very far back at
the word France that appeared in the
beginning of the sentence if we were
only looking at the past two words or
the past three words or even the past
five words it would be really hard to
guess the word in that blank so we don't
want to limit ourselves so much we want
to ideally use all of the information
that we have in the sequence but we also
need a fixed length vector so one way we
could do this is by using the entire
sequence but representing it as a set of
counts in language this representation
is also known as a bag of words all this
is is a vector in which each slot
represents a word and the number in that
slot represents the number of times that
that word occurs in the sentence
so here the second slot represents the
word this and there's a 1 because this
appears once in the sentence now we have
a fixed length vector no matter how many
words we have the vector will always be
the same size the counts will just be
different we can feed this into a
machine learning model and try to make a
prediction the problem you may be
noticing here is that we're losing all
of the sequential information these
counts don't preserve any order that we
had in the sequence to see why this is
really bad considered these two
sentences the food was good not bad at
all
versus the food was bad not good at all
these are completely opposite sentences
but their bag of words representation
would be exactly the same because they
contain the same set of words so by
representing our sentences counts we're
losing all of the sequential information
which is really important because we're
trying to model sequences ok so what do
we know now we want to preserve order in
the sequence but we also don't want to
cut it off to to a very short length you
might be saying well why don't we just
use a really big fixed window before we
were having issues because we were just
using a fixed window of size 2 what if
we extended that to be a fixed window of
size 7 and we think that by looking at 7
words we can get most of the context
that we need well yeah ok we can do that
now we have another fixed length vector
just like before it's bigger but it's
still fixed length we have allocated
space for each of the 7 words we can
feed this into a model and try to make a
prediction the problem here is that and
consider this in the scenario where
we're feeding this input vector into a
feed-forward neural network each of
those inputs each of those ones and
zeros has a separate weight connecting
it to the network if we see the words
this morning at the beginning of the
sentence very very commonly the network
will learn that this morning represents
a time or a setting if this morning then
appears at the end of the sentence we'll
have a lot of trouble recognizing that
because the weights at the end of the
vector never saw that phrase before and
the weights from the beginning of the
vector and not being shared with the end
in other words things we learn about the
sequence won't transfer if they appear
at different points in the sequence
were not sharing any parameters all
right so you kind of see all the
problems that arise with sequences now
and why we need a different framework
specifically we want to be able to deal
with variable length sequences we want
to maintain sequence order so we can
keep all about sequential information we
want to keep track of longer term
dependencies rather than cutting it off
too short and we want to be able to
share parameters across the sequence so
we don't have to relearn things across
the sequence because this is a class
about deep learning I'm gonna talk about
how to address these problems with
neural networks but know that time
series modeling and sequential modeling
is a very active field in machine
learning and it has been and there are
lots of other machine learning methods
that have been developed to deal with
these problems but for now I'll talk
about recurrent neural networks okay so
a recurrent neural network is
architected in the same way as a normal
neural network we have some inputs we
have some hidden layers and we have some
outputs the only difference is that each
hidden unit is doing a slightly
different function so let's take a look
at this one hidden unit to see exactly
what it's doing a recurrent hidden unit
computes a function of an input and its
own previous output its own previous
output is also known as the cell state
and in the diagram it's denoted by s the
subscript is the time step so at the
very first time stuff T equals zero the
recurrent unit computes a function of
the input at T equals zero and of its
initial state similarly at the next time
step it computes a function of the new
input and its previous cell state if you
look at the function at the bottom the
function to compute s 2 you'll see it's
really similar to the function for
consider unit in a feed-forward Network
computes the only difference is that
we're adding in an additional term to
incorporate its own previous state
a common way of viewing recurrent neural
networks is by unfolding them across
time so this is the same hidden unit at
different points in time here you can
see that at every point in time it takes
as input its own previous state and the
new input at that time step one thing to
notice here is that throughout the
sequence we're using the same weight
matrices W and u this solves our problem
of parameter sharing we don't have new
parameters for every point of the
sequence once we learn something it can
apply at any point in the sequence this
also helps us deal with variable length
sequences because we're not pre
specifying the length of the sequence we
don't have separate parameters for every
point in the sequence so in some cases
we could unroll this RNN to four time
steps in other cases we can unroll it to
ten time steps a final thing to notice
is that S sub n the self state at time n
can contain information from all of the
past time steps notice that each cell
state is a function of the previous self
state which is the function which is a
function of the previous cell state and
so on so this kind of solves our issue
of long-term dependencies because add a
time step very far in the future that
self state encompasses information about
all of the previous cell states all
right so now that you kind of understand
what a recurrent neural network is and
just to clarify I've shown you one
hidden unit in the previous slide but in
a full network you would have many many
of those hidden units and even many
layers of many hidden units so now we
can talk about how you would train a
recurrent neural network it's really
similar to how you train a normal neural
network it's back propagation there's
just an additional time dimension as a
reminder in back propagation we want to
find the parameters that minimize some
loss function the way that we do this is
by first taking the derivative of the
loss with respect to each of the
parameters and then shifting the
parameters in the opposite direction in
order to try and minimize the loss this
process is called gradient descent
so one difference with RNN is that we
have many time steps so we can produce
an output at every time step because we
have an output at every time step we can
have a loss at every time step rather
than just one single loss at the end
because and the way that we deal with
this is pretty simple the total loss is
just the sum of the losses at every time
step
similarly the total gradient is just the
sum of the gradients at every time step
so we can try this out by walking
through this gradient computation for a
single parameter W W is the weight
matrix that were multiplying by our
inputs we know that the total loss the
the total gradient so the derivative of
the loss with respect to W will be the
sum of the gradients at every time step
so for now we can focus on a single time
step knowing that at the end we would do
this for each of the time steps and then
sum them up to get the total gradient so
let's take time step two we can solve
this gradient using the chain rule so
the derivative of the loss with respect
to W is the derivative of the loss with
respect to the output the derivative of
the output with respect to the cell
state at time two and then the
derivative of the cell state with
respect to W so this seems fine but
let's take a closer look at this last
term you'll notice that s2 also depends
on s 1 and s 1 also depends on W so we
can't just leave that last term as a
constant we actually have to expand it
out farther ok so how do we expand this
out farther what we really want to know
is how exactly does the cell state at
time step 2 depend on W well it depends
directly on W because it feeds right in
we also saw that s 2 depends on s 1
which depends on W and you can also see
that s 2 depends on s 0 which also
depends on W in other words and here I'm
just writing it as a summation
those the the some that used on the
previous slide as a summation form and
you can see that the last two terms are
basically summing the contributions of W
in previous time steps to the error at
time step T this is key to how we model
longer term dependencies this gradient
is how we shift our parameters and our
parameters define our network by
shifting our parameters such that they
include contributions to the error from
past time steps they're shifted to model
longer term dependencies and here I'm
just writing it as a general sum not
just for time step two
okay so this is basically the process of
back propagation to through time you
would do this for every parameter in
your network and then use that in the
process of gradient descent in practice
ions are a bit difficult to train so I
kind of want to go through why that is
and what some ways some ways that we can
address these issues so let's go back to
this summation as a reminder this is the
derivative of the loss with respect to W
and this is what we would use to shift
our parameters W the last two terms are
considering the error of W at all of the
previous time steps let's take a look at
this one term this is how we this is the
derivative of the cell state at time
step two with respect to each of the
previous cell states you might notice
that this itself is also a chain rule
because s 2 depends on s 1 and s 1
depends on a zero we can expand this out
farther this is just for the derivative
of s 2 with respect to s 0 but what if
we were looking at a time step very far
in the future like time step n that term
would expand into a product of n terms
and ok you might be thinking so what
well as notice that as the gap between
time steps gets bigger and bigger this
product in the grade
gets longer and longer and if we look at
each of these terms what what are you -
these terms they all kind of take the
same form it's the derivative of a cell
state with respect to the previous cell
state that term can be written like this
and the actual form that actual formula
isn't that important just notice that
it's a product of two terms double use
and F Prime's
double use are our weight matrices these
are sampled mostly from a standard
normal distribution so most of the terms
will be less than one F prime is the
derivative of our activation function if
we use an activation function such as
the hyperbolic tangent or a sigmoid F
prime will always be less than one in
other words we're multiplying a lot of
small numbers together in this product
okay so what does this mean
basically recall that this product is
how we're adding the gradient from
future time steps to the gradient sorry
how we're adding the gradient from past
time steps to the gradient at a future
time step what's happening then is that
air is due to further and further back
time steps have increasingly smaller
gradients because that product for
further back time steps will be longer
and since the numbers are all decimals
they'll be it will be it will it'll
become increasingly smaller what this
what this ends up meaning at a high
level is that our parameters will become
biased to capture shorter term
dependencies the errors that arise from
further and further back time steps will
be harder and harder to propagate into
the gradient at future time steps recall
that recall this example that I showed
at the beginning the whole point of
using recurrent neural networks is
because we wanted to model long term
dependencies but if our parameters are
biased to capture short term
dependencies even if they see the whole
sequence they'll be but the parameters
will become biased to predict things
based mostly on the past couple words
okay so now I'm gonna go through some a
couple methods that are used to address
this issue in practice that work pretty
well the first one is the choice of
activation function so you saw that one
of the terms that was making that
product really small was the F prime
term F prime is the derivative of
whatever activation function we choose
to use here I've plotted the derivatives
of some common activation functions you
can see that the derivative of
hyperbolic tangent and sigmoid is always
less than one in fact for sigmoid it's
always less than 0.25 and instead we
choose to use an activation function
like relu it's always 1 above zero so
that will at least prevent the F prime
terms from shrinking the gradient
another solution would be how we
initialize our weights if we initialize
the weights from a normal distribution
they'll be mostly less than 1 and
they'll immediately shrink the gradients
if instead we initialize the weights to
something like the identity matrix it'll
at least prevent that W term from
shrinking that product at least at the
beginning the next solution is very
different it involves actually adding a
lot more complexity to the network using
a more complex type of cell called a
gated cell rather than here rather than
each node just being that simple our n n
unit that I showed at the beginning will
replace it with a much more complicated
cell a very common gated cell is
something called an L STM or a long
short term memory so like its name
implies L STM cells are able to keep
memory within the cell state unchanged
for many time steps this allows them to
effectively model longer-term
dependencies so I'm gonna go through a
very high-level overview of how Alice
TMS work but if you're interested feel
free to email me or ask me afterwards
and I can direct you to some more
resources to read about Alice gems in a
lot more detail alright
so Alice geum's basically you have a
three-step process the first step is to
forget
irrelevant parts of the cell state for
example if we're modeling a sentence and
we see a new subject we might want to
forget things about the old subject
because we know that future words will
be conjugated according to the new
subject the next state the next step is
an update step here's where we actually
update the cell state to reflect the new
the information from the new input in
this example like I said if we've just
seen a new subject we might want to this
is where we actually update the cell
state with the gender or whether the new
subject is plural or singular finally we
want to output certain parts of the cell
state so if we've just seen a subject we
have an idea that the next word might be
a verb so we'll output information
relevant to predicting a verb like the
tense each of these three steps is
implemented using a set of logic gates
and the logic gates are implemented
using sigmoid functions to give you some
intuition on ylst ms help with the
vanishing gradient problem is that first
the forget gate the first step can
equivalently be called the remember gate
because there you're choosing what to
forget and what to keep in the cell
state the forget gate can choose to keep
information in the cell state for many
many time steps
there's no activation function or
anything else shrinking that information
the second step the second thing is that
the cell state is separate from what's
outputted we're made this is not true of
normal recurrent units like I showed you
before in a simple recurrent unit the
cell state is the same thing as what
that cell outputs with an LS TM it has a
separate cell state and it only needs to
output information relevant to the
prediction at that time step because of
this it can keep information in the cell
state which might not be relevant at
this time stuff but might be relevant at
a much later time step so we can keep
that information without being penalized
for that
finally I didn't indicate this
explicitly in the diagram but the way
that the update step happens is through
an additive function not through a
multiplicative function so when we take
the
there's not a huge expansion so now I
just want to move on to going over some
possible tasks so the first task is
classification so here we want to
classify tweets as positive negative or
neutral and this task is also known as
sentiment analysis the way that we would
design a recurrent neural network to do
this is actually not by having an output
at every time step we only want one
output for the entire sequence and so
we'll take in the entire sequence the
entire tweet one word at a time and at
the very end we'll produce an output
which would which will actually be a
probability distribution over possible
classes where our classes in this case
would be positive negative or neutral
note that the only information that is
producing the output at the end is the
final cell state and so that final cell
state kind of has to summarize all of
the information from the begin the
entire sequence into that final cell
state so we can imagine if we have very
complicated tweets or well I don't know
if that's possible but very complicated
paragraphs or sentences
we might want to create a bigger network
with more hidden states to allow that
last state to be more expressive the
next task would be something like music
generation and I'll see if this will
play you can kind of hear it
okay so that was music generated by an
RNN which is pretty cool and something
you're actually also gonna do in the lab
today but music generator you can on RNN
can produce music because music is just
a sequence and the way that you would
the way that you would construct a new
recurrent neural network to do this
would be at every time point taking in a
note and producing the most likely next
note given the notes that you've seen so
far so here you would produce an output
at every time step the final task is
machine translation machine translation
is interesting because it's actually two
recurrent neural networks side-by-side
the first is an encoder the encoder
takes as input a sentence in a source
language like English it then there's
done a decoder which produces the same
sentence in a target language like
French notice in this architecture that
the only information passed from the
encoder to the decoder is the final cell
state and the idea is that that final
state should be kind of a summary of the
entire encoder sentence and given that
summary the decoder should be able to
figure out what the encoder sentence was
about and then produce the same sentence
in a different language you can imagine
though that okay maybe this is possible
for a sentence a really simple sentence
like the dog eats maybe we can encode
that in the final cell state but if we
had a much more complicated sentence or
a much longer sentence that would be
very difficult to try and summarize the
whole thing in that one cell state so
what's typically done in practice for a
machine translation is something called
attention with attention rather than
just taking in the final cell state to
the decoder at each time step we take in
a weighted sum of all of the previous
cell states so in this case we're trying
to produce the first word we'll take in
a weighted sum of all of the encoder
States most of the weight will probably
be on the first state because that's
what would be most relevant to producing
the first
then when we produced the second word
most of the weight will probably be on
the second cell state but we might have
some on the first and the third to try
and get an idea for the tenths or the
gender of this now and the same thing
for all of the cell states the way that
you implement this is just by including
those weight parameters in the weighted
sum as additional parameters that you
train using back propagation just like
everything else okay so I hope that you
have an idea now why we need a different
framework to model sequences and how
recurrent neural networks can solve some
of the issues that we saw at the
beginning as well as an idea of how to
train them and solve some of the
vanishing gradient problems I've talked
a lot about language but you can imagine
using these exact same neural net
recurrent neural networks for modeling
time series or waveforms or doing other
interesting sequence prediction tasks
like predicting stock market trends or
summarizing books or articles and maybe
you'll consider some sequence modeling
tasks for your final project so thank
you
[Applause]
