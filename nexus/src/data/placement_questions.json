[
  {
    "id": 1,
    "question": "What is the result of multiplying a 2×3 matrix by a 3×1 vector?",
    "choices": ["A 2×1 vector","A 3×2 matrix","A 1×3 vector","A 2×3 matrix"],
    "answerIndex": 0,
    "bracket": "beginner",
    "nodeId": "matrix_multiplication"
  },
  {
    "id": 2,
    "question": "Which operation computes the dot product of two equal-length vectors?",
    "choices": ["Element-wise multiplication","Sum of element-wise products","Outer product","Cross product"],
    "answerIndex": 1,
    "bracket": "beginner",
    "nodeId": "dot_product"
  },
  {
    "id": 3,
    "question": "What does the softmax function produce when applied to a vector?",
    "choices": ["Binary outputs","A probability distribution","Unnormalized scores","Sparse vector"],
    "answerIndex": 1,
    "bracket": "beginner",
    "nodeId": "softmax"
  },
  {
    "id": 4,
    "question": "What is one-hot encoding?",
    "choices": ["Mapping categories to scalars","Mapping categories to orthogonal binary vectors","Dimensionality reduction","Normalization method"],
    "answerIndex": 1,
    "bracket": "beginner",
    "nodeId": "one_hot_encoding"
  },
  {
    "id": 5,
    "question": "Why are embedding layers used in NLP models?",
    "choices": ["To reduce vocabulary size","To map tokens into continuous vector space","To normalize inputs","To implement attention"],
    "answerIndex": 1,
    "bracket": "beginner",
    "nodeId": "embeddings"
  },
  {
    "id": 6,
    "question": "What is gradient descent used for?",
    "choices": ["Data preprocessing","Optimizing model parameters","Model evaluation","Feature extraction"],
    "answerIndex": 1,
    "bracket": "beginner",
    "nodeId": "gradient_descent"
  },
  {
    "id": 7,
    "question": "Which variant of gradient descent updates parameters per batch?",
    "choices": ["Batch GD","Stochastic GD","Mini-batch GD","Coordinate descent"],
    "answerIndex": 2,
    "bracket": "intermediate",
    "nodeId": "mini_batch_gd"
  },
  {
    "id": 8,
    "question": "What is the role of the learning rate in optimization?",
    "choices": ["Controls model depth","Scales gradient step size","Adjusts batch size","Determines regularization"],
    "answerIndex": 1,
    "bracket": "intermediate",
    "nodeId": "learning_rate"
  },
  {
    "id": 9,
    "question": "Which optimizer uses running estimates of first and second moments of gradients?",
    "choices": ["SGD","Momentum","RMSprop","Adam"],
    "answerIndex": 3,
    "bracket": "advanced",
    "nodeId": "adam_optimizer"
  },
  {
    "id": 10,
    "question": "What is dropout regularization?",
    "choices": ["Adding noise to inputs","Randomly zeroing activations during training","Reducing learning rate","Increasing model depth"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "dropout"
  },
  {
    "id": 11,
    "question": "Why apply layer normalization in deep networks?",
    "choices": ["Stabilize activations","Reduce parameter count","Improve dropout","Decrease depth"],
    "answerIndex": 0,
    "bracket": "advanced",
    "nodeId": "layer_normalization"
  },
  {
    "id": 12,
    "question": "What is a residual connection?",
    "choices": ["Concatenating inputs and outputs","Adding input to sublayer output","Multiplying layers","Applying softmax"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "residual_connection"
  },
  {
    "id": 13,
    "question": "Which common activation appears in feed-forward networks?",
    "choices": ["Softmax","ReLU","Sigmoid","Tanh"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "activation_functions"
  },
  {
    "id": 14,
    "question": "What is the time complexity of multiplying two n×n matrices with the naïve algorithm?",
    "choices": ["O(n)","O(n log n)","O(n²)","O(n³)"],
    "answerIndex": 3,
    "bracket": "advanced",
    "nodeId": "matrix_complexity"
  },
  {
    "id": 15,
    "question": "What is sequence transduction?",
    "choices": ["Mapping input sequence to label","Mapping one sequence to another","Adding sequences","Shuffling sequences"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "sequence_transduction"
  },
  {
    "id": 16,
    "question": "How does a recurrent neural network process sequences?",
    "choices": ["In parallel","Token by token with hidden state","Ignoring order","Using convolution"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "rnn"
  },
  {
    "id": 17,
    "question": "Which layer performs local pattern detection on sequences/images?",
    "choices": ["Recurrent layer","Convolutional layer","Self-attention","Normalization layer"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "cnn"
  },
  {
    "id": 18,
    "question": "What role do encoder and decoder play in sequence models?",
    "choices": ["Encoder generates output; decoder processes input","Encoder processes input; decoder generates output","Both classify","Both embed"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "encoder_decoder"
  },
  {
    "id": 19,
    "question": "What is an attention mechanism’s core idea?",
    "choices": ["Weight inputs by learned scores","Apply convolution","Use recurrence","Normalize gradients"],
    "answerIndex": 0,
    "bracket": "advanced",
    "nodeId": "attention_mechanism"
  },
  {
    "id": 20,
    "question": "How is scaled dot-product attention computed?",
    "choices": ["Softmax(QKᵀ)·V","Softmax(QKᵀ/√dₖ)·V","ReLU(Q+K)·V","Sigmoid(QKᵀ)·V"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "scaled_attention"
  },
  {
    "id": 21,
    "question": "What does parallelization refer to in model training?",
    "choices": ["Sequential updates","Simultaneous computation across devices","Single-threaded loops","Layer stacking"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "parallelization"
  },
  {
    "id": 22,
    "question": "Why are GPUs favored for deep learning?",
    "choices": ["Higher clock speed","Optimized for massive parallel operations","Lower power consumption","Larger memory per core"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "gpu_architecture"
  },
  {
    "id": 23,
    "question": "What is the BLEU score used to evaluate?",
    "choices": ["Image quality","Translation quality","Parsing accuracy","Speech synthesis"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "bleu_score"
  },
  {
    "id": 24,
    "question": "Which technique combines multiple models’ predictions?",
    "choices": ["Regularization","Ensembling","Dropout","Normalization"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "ensemble_methods"
  },
  {
    "id": 25,
    "question": "What is label smoothing?",
    "choices": ["Random label changes","Adding uniform noise to targets","Weighted sampling","Gradient clipping"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "label_smoothing"
  },
  {
    "id": 26,
    "question": "How does a mask in attention work?",
    "choices": ["Zeroes out attention weights","Adds large negative values to logits","Normalizes scores","Concatenates vectors"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "attention_mask"
  },
  {
    "id": 27,
    "question": "What is beam search used for?",
    "choices": ["Training optimization","Sequence decoding","Feature extraction","Regularization"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "beam_search"
  },
  {
    "id": 28,
    "question": "What is constituency parsing?",
    "choices": ["Assigning part-of-speech tags","Building tree structure for sentence","Translating text","Summarizing text"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "constituency_parsing"
  },
  {
    "id": 29,
    "question": "Generalization refers to a model’s ability to:",
    "choices": ["Fit training data perfectly","Perform well on unseen data","Have low parameter count","Train faster"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "generalization"
  },
  {
    "id": 30,
    "question": "What causes overfitting?",
    "choices": ["Insufficient model capacity","Excessive training data","Model too complex relative to data","High learning rate"],
    "answerIndex": 2,
    "bracket": "advanced",
    "nodeId": "overfitting"
  },
  {
    "id": 31,
    "question": "What is the purpose of weight initialization?",
    "choices": ["Ensure reproducibility","Break symmetry and stabilize gradients","Reduce memory usage","Increase depth"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "weight_initialization"
  },
  {
    "id": 32,
    "question": "Why use an inverse-square-root learning rate schedule?",
    "choices": ["Faster convergence","Simpler implementation","Warmup then decay","Avoid overfitting"],
    "answerIndex": 2,
    "bracket": "advanced",
    "nodeId": "lr_schedule"
  },
  {
    "id": 33,
    "question": "What is the computational complexity of self-attention for sequence length n?",
    "choices": ["O(n)","O(n log n)","O(n²)","O(n³)"],
    "answerIndex": 2,
    "bracket": "advanced",
    "nodeId": "attention_complexity"
  },
  {
    "id": 34,
    "question": "What is sparse attention?",
    "choices": ["Zeroing small weights","Restricting attention to subsets of positions","Using sparse matrices for embeddings","Removing normalization"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "sparse_attention"
  },
  {
    "id": 35,
    "question": "Relative positional encoding differs from absolute by:",
    "choices": ["Encoding distance between tokens","Using fixed sinusoidal vectors","Concatenating positions","Ignoring order"],
    "answerIndex": 0,
    "bracket": "advanced",
    "nodeId": "positional_encoding_relative"
  },
  {
    "id": 36,
    "question": "What does KL divergence measure between distributions?",
    "choices": ["Euclidean distance","Similarity","Discrepancy","Correlation"],
    "answerIndex": 2,
    "bracket": "advanced",
    "nodeId": "kl_divergence"
  },
  {
    "id": 37,
    "question": "Transfer learning allows a model to:",
    "choices": ["Train from scratch faster","Reuse parameters on new tasks","Decrease memory usage","Avoid fine-tuning"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "transfer_learning"
  },
  {
    "id": 38,
    "question": "Why might very deep post-layer-norm Transformers be unstable?",
    "choices": ["Excessive parameter count","Gradient vanishing/explosion","Lack of dropout","Small batch sizes"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "training_stability"
  },
  {
    "id": 39,
    "question": "What is the main memory bottleneck for long-sequence self-attention?",
    "choices": ["Embedding lookup","Computing QKᵀ","Feed-forward layers","Layer norm"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "memory_bottleneck"
  },
  {
    "id": 40,
    "question": "How can training costs be reduced in large models?",
    "choices": ["Increase batch size only","Use mixed-precision and parallelism","Remove attention","Use larger learning rate"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "training_costs"
  },
  {
    "id": 41,
    "question": "What is ensemble distillation?",
    "choices": ["Combining models at inference","Transferring ensemble knowledge to a single model","A type of dropout","Parallel training strategy"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "ensemble_distillation"
  },
  {
    "id": 42,
    "question": "What is the purpose of gradient clipping?",
    "choices": ["Accelerate training","Prevent exploding gradients","Improve generalization","Normalize activations"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "gradient_clipping"
  },
  {
    "id": 43,
    "question": "What is zero-shot generalization?",
    "choices": ["Generalizing with no training","Generalizing on unseen classes without fine-tuning","Fine-tuning on new data","Zero loss on test"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "zero_shot"
  },
  {
    "id": 44,
    "question": "Which metric captures translation adequacy and fluency beyond BLEU?",
    "choices": ["ROUGE","METEOR","Accuracy","Perplexity"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "translation_metrics"
  },
  {
    "id": 45,
    "question": "How does label smoothing relate to minimizing cross-entropy?",
    "choices": ["Removes cross-entropy term","Adds uniform noise to target distribution","Maximizes cross-entropy","Ignores true labels"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "label_smoothing_ce"
  },
  {
    "id": 46,
    "question": "What is dynamic masking in attention?",
    "choices": ["Changing masks per layer","Using fixed masks","Random dropout of keys","Masking future tokens only"],
    "answerIndex": 0,
    "bracket": "advanced",
    "nodeId": "dynamic_masking"
  },
  {
    "id": 47,
    "question": "Why is self-attention more parallelizable than recurrence?",
    "choices": ["Processes tokens sequentially","No hidden state dependency","Requires fewer parameters","Uses convolution"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "parallel_attention"
  },
  {
    "id": 48,
    "question": "What is the impact of tokenization granularity on model performance?",
    "choices": ["No impact","Trade-off between vocabulary size and sequence length","Only affects embedding size","Only affects attention"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "tokenization"
  },
  {
    "id": 49,
    "question": "Which factor most influences GPU training throughput?",
    "choices": ["Sequence length","Batch size and model parallelism","Optimizer choice","Dropout rate"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "gpu_throughput"
  },
  {
    "id": 50,
    "question": "What is domain adaptation in NLP?",
    "choices": ["Training on source and target combined","Fine-tuning a pre-trained model on target domain data","Removing domain-specific tokens","Using zero-shot transfer"],
    "answerIndex": 1,
    "bracket": "advanced",
    "nodeId": "domain_adaptation"
  }
]
