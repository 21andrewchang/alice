{
  "nodes": [
    {
      "id": 0,
      "label": "Attention Is All You Need",
      "type": "paper",
      "description": "Introduces the Transformer architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions. This paper revolutionized natural language processing and became the foundation for modern large language models.",
      "difficulty": 4,
      "domain": "research-papers",
      "isPaper": true,
      "url": "attention_is_all_you_need.pdf",
      "content": {
        "abstract": "The dominant <node id=\"3\">Sequence Transduction Model</node>s are based on complex <node id=\"5\">Recurrent Neural Networks</node> or <node id=\"15\">Convolutional Neural Networks</node> that include an <node id=\"6\">Encoder</node> and a <node id=\"7\">Decoder</node>. The best performing models also connect the encoder and decoder through an <node id=\"4\">Attention Mechanisms</node>. We propose a new simple network architecture, the <node id=\"10\">Transformer</node>, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two <node id=\"20\">Machine Translation</node> tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight <node id=\"26\">GPU</node>s, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "introduction": "<node id=\"5\">Recurrent Neural Networks</node>, <node id=\"17\">Long Short Term Memory</node> and <node id=\"18\">Gated Recurrent Neural Network</node>s in particular, have been firmly established as state of the art approaches in <node id=\"19\">Sequence Modeling</node> and transduction problems such as <node id=\"21\">Language Modeling</node> and <node id=\"20\">Machine Translation</node>. Numerous efforts have since continued to push the boundaries of recurrent language models and <node id=\"6\">Encoder</node>-<node id=\"7\">Decoder</node> architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of <node id=\"22\">Hidden States</node>, as a function of the previous hidden state and the input for position t. This inherently sequential nature precludes <node id=\"24\">Parallelization</node> within training examples, which becomes critical at longer sequence lengths, as memory constraints limit <node id=\"27\">Batching</node> across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. <node id=\"4\">Attention Mechanisms</node> have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of <node id=\"28\">Dependencies</node> without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the <node id=\"10\">Transformer</node>, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight <node id=\"26\">GPU</node>s.",
        "model_architecture": "Most competitive neural <node id=\"3\">Sequence Transduction Model</node>s have an <node id=\"6\">Encoder</node>-<node id=\"7\">Decoder</node> structure. Here, the encoder maps an input sequence of symbol representations to a sequence of continuous representations. Given this, the decoder then generates an output sequence of symbols one element at a time. At each step the model is <node id=\"23\">Auto-Regressive</node>, consuming the previously generated symbols as additional input when generating the next.\n\n**Encoder:** The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a <node id=\"9\">Self Attention</node> mechanism, and the second is a simple, position-wise fully connected <node id=\"12\">Feed-forward Network</node>. We employ a residual connection around each of the two sub-layers, followed by <node id=\"13\">Layer Normalization</node>. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the <node id=\"14\">Embedding</node> layers, produce outputs of dimension d_model = 512.\n\n**Decoder:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs <node id=\"8\">Multi-Head Attention</node> over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n\n**Attention:** An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n**Scaled Dot-Product Attention:** The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a <node id=\"34\">Softmax Function</node> to obtain the weights on the values.\n\n**Multi-Head Attention:** Instead of performing a single attention function with d_model-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different learned projections. On each of these projected versions, we perform the attention function in parallel, yielding output values. These are concatenated and once again projected, resulting in the final values. In this work, we employ h = 8 heads, with d_k = d_v = d_model/h = 64.\n\nThe Transformer uses multi-head attention in three ways: (1) Encoder–Decoder Attention, (2) Encoder Self-Attention, and (3) Decoder Self-Attention (masked).\n\n**Feed-Forward Networks:** In addition to attention sub-layers, each layer in our encoder and decoder contains a fully connected feed-forward network applied to each position separately.\n\n**Embeddings and Softmax:** We use learned <node id=\"14\">Embedding</node>s to convert tokens to vectors of dimension d_model and share the same weight matrix between the input embedding, output embedding, and pre-softmax linear transformation. Embeddings are scaled by sqrt(d_model).\n\n**Positional Encoding:** Because the model has no recurrence or convolution, we inject positional information via sinusoidal encodings added to the embeddings.",
        "why_self_attention": "We compare <node id=\"9\">Self Attention</node>, recurrent, and convolutional layers by: 1. Computational complexity per layer 2. Parallelizability (minimum sequential operations) 3. Maximum path length for long-range dependencies. Self-attention has O(n^2 · d) complexity, O(1) sequential operations, and O(1) maximum path length.",
        "training": "We trained on WMT 2014 English–German (4.5M sentence pairs) and WMT 2014 English–French (36M sentences) datasets. Batches contain ~25K source and target tokens. We used Adam optimizer with learning rate scheduling and applied residual dropout and label smoothing for regularization.",
        "results": "The <node id=\"10\">Transformer</node> achieves 28.4 BLEU on EN→DE and 41.8 BLEU on EN→FR, outperforming prior models at lower training cost. The model also achieved state-of-the-art results on English constituency parsing.",
        "conclusion": "We presented the <node id=\"10\">Transformer</node>, the first <node id=\"3\">Sequence Transduction Model</node> based entirely on <node id=\"4\">Attention Mechanisms</node>, replacing recurrent and convolutional layers with <node id=\"9\">Self Attention</node>. It parallelizes efficiently, trains faster, and sets new state-of-the-art on translation and parsing tasks.",
        "original_paper_url": "https://arxiv.org/abs/1706.03762"
      }
    },
    {
      "id": 1,
      "label": "Neural Networks",
      "type": "concept",
      "description": "Computational models inspired by biological neural networks, consisting of interconnected nodes that process information.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 2,
      "label": "Backpropagation",
      "type": "concept",
      "description": "Algorithm for training neural networks by calculating gradients and updating weights through reverse differentiation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 3,
      "label": "Sequence-to-Sequence Models",
      "type": "concept",
      "description": "Neural architectures that transform input sequences into output sequences, commonly used in translation and text generation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 4,
      "label": "Attention Mechanisms",
      "type": "concept",
      "description": "Techniques that allow models to focus on relevant parts of input when making predictions.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 5,
      "label": "Recurrent Neural Networks",
      "type": "concept",
      "description": "Neural networks with loops that allow information to persist, designed for sequential data processing.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 6,
      "label": "Encoder-Decoder Architecture",
      "type": "concept",
      "description": "Neural network design pattern where an encoder processes input and a decoder generates output.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 7,
      "label": "Multi-Head Attention",
      "type": "concept",
      "description": "Extension of attention mechanism that allows the model to jointly attend to information from different representation subspaces.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 8,
      "label": "Positional Encoding",
      "type": "concept",
      "description": "Method to inject information about the relative or absolute position of tokens in a sequence.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 9,
      "label": "Linear Algebra",
      "type": "concept",
      "description": "Mathematical foundation involving vectors, matrices, and linear transformations essential for understanding neural networks.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 10,
      "label": "Calculus",
      "type": "concept",
      "description": "Mathematical foundation for understanding derivatives and gradients used in neural network training.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 11,
      "label": "Probability Theory",
      "type": "concept",
      "description": "Mathematical framework for handling uncertainty and randomness in machine learning models.",
      "difficulty": 2,
      "domain": "math"
    }
  ],
  "links": [
    {
      "source": 1,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 2,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 3,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 7,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 8,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 1,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 10,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 11,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 6,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 4,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 7,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 8,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 5,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 6,
      "relation": "prerequisite",
      "value": 1
    }
  ]
} 