{
  "nodes": [
    {
      "id": 0,
      "label": "Attention Is All You Need",
      "type": "paper",
      "description": "Introduces the Transformer architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions. This paper revolutionized natural language processing and became the foundation for modern large language models.",
      "difficulty": 4,
      "domain": "research-papers",
      "isPaper": true,
      "url": "attention_is_all_you_need.pdf",
      "content": {
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the <node id=\"10\">Transformer</node>, based solely on <node id=\"4\">attention mechanisms</node>, dispensing with recurrence and convolutions entirely. Experiments on two <node id=\"20\">machine translation</node> tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.",
        "introduction": "Recurrent neural networks, <node id=\"17\">long short-term memory</node> and <node id=\"18\">gated recurrent</node> neural networks in particular, have been firmly established as state of the art approaches in <node id=\"19\">sequence modeling</node> and transduction problems such as <node id=\"21\">language modeling</node> and <node id=\"20\">machine translation</node>.\n\n<node id=\"4\">Attention mechanisms</node> have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases, however, such attention mechanisms are used in conjunction with a recurrent network.\n\nIn this work we propose the <node id=\"10\">Transformer</node>, a model architecture eschewing recurrence and instead relying entirely on an <node id=\"4\">attention mechanism</node> to draw global dependencies between input and output. The Transformer allows for significantly more <node id=\"24\">parallelization</node> and can reach a new state of the art in translation quality.",
        "model_architecture": "Most competitive neural sequence transduction models have an <node id=\"6\">encoder</node>-<node id=\"7\">decoder</node> structure. Here, the <node id=\"6\">encoder</node> maps an input sequence of symbol representations to a sequence of continuous representations. Given this representation, the <node id=\"7\">decoder</node> then generates an output sequence of symbols one element at a time. At each step the model is <node id=\"23\">auto-regressive</node>, consuming the previously generated symbols as additional input.\n\n### Encoder and Decoder Stacks\n\n**Encoder:** The <node id=\"6\">encoder</node> is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a <node id=\"8\">multi-head self-attention</node> mechanism, and the second is a simple, position-wise fully connected <node id=\"12\">feed-forward network</node>. We employ a residual connection around each of the two sub-layers, followed by <node id=\"13\">layer normalization</node>.\n\n**Decoder:** The <node id=\"7\">decoder</node> is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs <node id=\"8\">multi-head attention</node> over the output of the encoder stack.\n\n### Attention\n\nAn <node id=\"4\">attention function</node> can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n#### Scaled Dot-Product Attention\n\nWe call our particular attention \"Scaled Dot-Product Attention\". The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the <node id=\"27\">dot products</node> of the query with all keys, divide each by √d_k, and apply a <node id=\"28\">softmax function</node> to obtain the weights on the values.\n\n#### Multi-Head Attention\n\nInstead of performing a single attention function with d_model-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different learned projections. On each of these projected versions, we perform the attention function in parallel, yielding d_v-dimensional output values. These are concatenated and once again projected, resulting in the final values.\n\nThe <node id=\"10\">Transformer</node> uses <node id=\"8\">multi-head attention</node> in three ways:\n\n- **Encoder–Decoder Attention:** queries come from the previous decoder layer, keys and values come from the encoder output\n- **Encoder Self-Attention:** queries, keys, and values all come from the previous encoder layer  \n- **Decoder Self-Attention (masked):** same-layer queries, keys, and values with masking to prevent attending to future positions\n\n### Position-wise Feed-Forward Networks\n\nIn addition to attention sub-layers, each layer in our <node id=\"6\">encoder</node> and <node id=\"7\">decoder</node> contains a fully connected <node id=\"12\">feed-forward network</node> applied to each position separately.\n\n### Embeddings and Softmax\n\nWe use learned <node id=\"14\">embeddings</node> to convert tokens to vectors of dimension d_model and share the same weight matrix between the input embedding, output embedding, and pre-softmax linear transformation.\n\n### Positional Encoding\n\nBecause the model has no recurrence or convolution, we inject positional information via sinusoidal encodings added to the <node id=\"14\">embeddings</node>:\n\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))",
        "why_self_attention": "We compare <node id=\"9\">self-attention</node>, recurrent, and convolutional layers by:\n\n1. Computational complexity per layer\n2. Parallelizability (minimum sequential operations)  \n3. Maximum path length for long-range dependencies\n\n<node id=\"9\">Self-attention</node> has O(n² · d) complexity, O(1) sequential operations, and O(1) maximum path length.",
        "training": "We trained on WMT 2014 English–German (4.5M sentence pairs) and WMT 2014 English–French (36M sentences) datasets. We used Adam optimizer with learning rate scheduling and applied residual dropout and label smoothing for regularization.",
        "results": "<node id=\"10\">Transformer</node> (big) achieves 28.4 BLEU on EN→DE and 41.8 BLEU on EN→FR, outperforming prior models at lower training cost. The model also achieved state-of-the-art results on English constituency parsing.",
        "conclusion": "We presented the <node id=\"10\">Transformer</node>, the first sequence transduction model based entirely on attention, replacing recurrent and convolutional layers with <node id=\"8\">multi-headed self-attention</node>. It parallelizes efficiently, trains faster, and sets new state-of-the-art on translation and parsing tasks.",
        "original_paper_url": "https://arxiv.org/abs/1706.03762"
      }
    },
    {
      "id": 1,
      "label": "Neural Networks",
      "type": "concept",
      "description": "Computational models inspired by biological neural networks, consisting of interconnected nodes that process information.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 2,
      "label": "Backpropagation",
      "type": "concept",
      "description": "Algorithm for training neural networks by calculating gradients and updating weights through reverse differentiation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 3,
      "label": "Sequence Transduction Model",
      "type": "concept",
      "description": "Neural architectures that transform input sequences into output sequences, the core problem addressed by the Transformer.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 4,
      "label": "Attention Mechanisms",
      "type": "concept",
      "description": "Techniques that allow models to focus on relevant parts of input when making predictions.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 5,
      "label": "Recurrent Neural Networks",
      "type": "concept",
      "description": "Neural networks with loops that allow information to persist, designed for sequential data processing.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 6,
      "label": "Encoder",
      "type": "concept",
      "description": "Component that processes input sequence and creates a fixed-size representation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 7,
      "label": "Decoder",
      "type": "concept",
      "description": "Component that generates output sequence from the encoded representation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 8,
      "label": "Multi-Head Attention",
      "type": "concept",
      "description": "Extension of attention mechanism that allows the model to jointly attend to information from different representation subspaces.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 9,
      "label": "Self Attention",
      "type": "concept",
      "description": "Attention mechanism where queries, keys, and values all come from the same sequence.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 10,
      "label": "Transformer",
      "type": "concept",
      "description": "Neural network architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 11,
      "label": "Positional Encoding",
      "type": "concept",
      "description": "Method to inject information about the relative or absolute position of tokens in a sequence.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 12,
      "label": "Feed-forward Network",
      "type": "concept",
      "description": "Position-wise fully connected layers that process each position separately and identically.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 13,
      "label": "Layer Normalization",
      "type": "concept",
      "description": "Normalization technique applied within each layer to stabilize training.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 14,
      "label": "Embedding",
      "type": "concept",
      "description": "Dense vector representations of discrete tokens like words or subwords.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 15,
      "label": "Convolutional Neural Networks",
      "type": "concept",
      "description": "Neural networks specialized for processing grid-like data, using convolution operations.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 16,
      "label": "Convolution",
      "type": "concept",
      "description": "Mathematical operation that applies filters to local regions of input data.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 17,
      "label": "Long Short Term Memory",
      "type": "concept",
      "description": "Type of RNN designed to avoid vanishing gradient problems through gating mechanisms.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 18,
      "label": "Gated Recurrent Neural Network",
      "type": "concept",
      "description": "RNN variant that uses gates to control information flow, simpler than LSTM.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 19,
      "label": "Sequence Modeling",
      "type": "concept",
      "description": "Techniques for processing and predicting sequential data like text or time series.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 20,
      "label": "Machine Translation",
      "type": "concept",
      "description": "Automated translation of text from one language to another using computational methods.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 21,
      "label": "Language Modeling",
      "type": "concept",
      "description": "Statistical modeling of natural language to predict word sequences and text patterns.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 22,
      "label": "Hidden States",
      "type": "concept",
      "description": "Internal representations that capture information processed at each step in sequence models.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 23,
      "label": "Auto-Regressive",
      "type": "concept",
      "description": "Generation method where each output depends on previously generated outputs.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 24,
      "label": "Parallelization",
      "type": "concept",
      "description": "Ability to process multiple computations simultaneously, key advantage of Transformers.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 25,
      "label": "Training a Model",
      "type": "concept",
      "description": "Process of optimizing neural network parameters using data and loss functions.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 26,
      "label": "GPU",
      "type": "concept",
      "description": "Graphics Processing Unit optimized for parallel computations, essential for training large models.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 27,
      "label": "Batching",
      "type": "concept",
      "description": "Processing multiple examples simultaneously to improve computational efficiency.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 28,
      "label": "Dependencies",
      "type": "concept",
      "description": "Relationships between elements in sequences that models need to capture.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 29,
      "label": "ConvS2S",
      "type": "concept",
      "description": "Convolutional sequence-to-sequence model, predecessor to Transformer architecture.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 30,
      "label": "ByteNet",
      "type": "concept",
      "description": "Convolutional neural network for sequence modeling with dilated convolutions.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 31,
      "label": "Matrix Multiplication",
      "type": "concept",
      "description": "Mathematical operation for combining two matrices, fundamental to neural network computations.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 32,
      "label": "Dot Product",
      "type": "concept",
      "description": "Mathematical operation that multiplies corresponding elements of vectors and sums them, used in attention scoring.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 33,
      "label": "Vector Operations",
      "type": "concept",
      "description": "Basic mathematical operations on vectors like addition, subtraction, and scalar multiplication.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 34,
      "label": "Softmax Function",
      "type": "concept",
      "description": "Activation function that converts a vector of real numbers into a probability distribution.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 35,
      "label": "Gradient",
      "type": "concept",
      "description": "Vector of partial derivatives indicating the direction and rate of steepest increase of a function.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 36,
      "label": "Derivative",
      "type": "concept",
      "description": "Mathematical concept measuring how a function changes as its input changes.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 37,
      "label": "Chain Rule",
      "type": "concept",
      "description": "Mathematical rule for computing the derivative of composite functions, essential for backpropagation.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 38,
      "label": "Partial Derivatives",
      "type": "concept",
      "description": "Derivatives of functions with multiple variables, taken with respect to one variable while holding others constant.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 39,
      "label": "Trigonometric Functions",
      "type": "concept",
      "description": "Mathematical functions like sine and cosine used in positional encoding for sequence position.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 40,
      "label": "Mean",
      "type": "concept",
      "description": "Average value of a set of numbers, used in normalization techniques.",
      "difficulty": 1,
      "domain": "math"
    },
    {
      "id": 41,
      "label": "Variance",
      "type": "concept",
      "description": "Measure of how spread out numbers are from their mean, used in layer normalization.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 42,
      "label": "Linear Transformation",
      "type": "concept",
      "description": "Mathematical mapping between vector spaces that preserves vector addition and scalar multiplication.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 43,
      "label": "Weighted Sum",
      "type": "concept",
      "description": "Sum of values each multiplied by a corresponding weight, fundamental to neural network operations.",
      "difficulty": 2,
      "domain": "math"
    }
  ],
  "links": [
    {
      "source": 1,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 2,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 10,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 8,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 11,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 43,
      "target": 1,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 1,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 35,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 37,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 32,
      "target": 4,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 34,
      "target": 4,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 5,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 6,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 7,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 8,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 42,
      "target": 8,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 9,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 6,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 7,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 8,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 11,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 12,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 13,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 39,
      "target": 11,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 11,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 43,
      "target": 12,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 42,
      "target": 12,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 40,
      "target": 13,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 41,
      "target": 13,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 14,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 16,
      "target": 15,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 16,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 17,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 18,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 19,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 3,
      "target": 20,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 21,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 22,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 7,
      "target": 23,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 26,
      "target": 24,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 2,
      "target": 25,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 27,
      "target": 25,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 28,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 15,
      "target": 29,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 3,
      "target": 29,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 15,
      "target": 30,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 31,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 32,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 35,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 38,
      "target": 35,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 37,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 38,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 40,
      "target": 41,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 42,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 43,
      "relation": "prerequisite",
      "value": 1
    }
  ]
} 