{
  "nodes": [
    {
      "id": 0,
      "label": "Attention Is All You Need",
      "type": "paper",
      "description": "Introduces the Transformer architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions. This paper revolutionized natural language processing and became the foundation for modern large language models.",
      "difficulty": 4,
      "domain": "research-papers",
      "isPaper": true,
      "url": "attention_is_all_you_need.pdf",
      "content": {
        "abstract": "The dominant <node id=\"3\">sequence transduction models</node> are based on complex <node id=\"5\">recurrent</node> or <node id=\"15\">convolutional neural networks</node> that include an <node id=\"6\">encoder</node> and a <node id=\"7\">decoder</node>. The best performing models also connect the encoder and decoder through an <node id=\"4\">attention mechanism</node>. We propose a new simple network architecture, the <node id=\"10\">Transformer</node>, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two <node id=\"20\">machine translation</node> tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight <node id=\"26\">GPUs</node>, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "introduction": "<node id=\"5\">Recurrent neural networks</node>, <node id=\"17\">long short-term memory</node> [13] and <node id=\"18\">gated recurrent</node> [7] neural networks in particular, have been firmly established as state of the art approaches in <node id=\"19\">sequence modeling</node> and transduction problems such as <node id=\"21\">language modeling</node> and <node id=\"20\">machine translation</node> [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and <node id=\"6\">encoder</node>-<node id=\"7\">decoder</node> architectures [38, 24, 15].\n\n<node id=\"5\">Recurrent models</node> typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of <node id=\"22\">hidden states</node> h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes <node id=\"24\">parallelization</node> within training examples, which becomes critical at longer sequence lengths, as memory constraints limit <node id=\"27\">batching</node> across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n\n<node id=\"4\">Attention mechanisms</node> have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of <node id=\"28\">dependencies</node> without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n\nIn this work we propose the <node id=\"10\">Transformer</node>, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 <node id=\"26\">GPUs</node>.",
        "background": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], <node id=\"30\">ByteNet</node> [18] and <node id=\"29\">ConvS2S</node> [9], all of which use <node id=\"15\">convolutional neural networks</node> as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn <node id=\"28\">dependencies</node> between distant positions [12]. In the <node id=\"10\">Transformer</node> this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with <node id=\"8\">Multi-Head Attention</node> as described in Section 3.2.\n\n<node id=\"9\">Self-attention</node>, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. <node id=\"9\">Self-attention</node> has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and <node id=\"21\">language modeling</node> tasks [34].\n\nTo the best of our knowledge, however, the <node id=\"10\">Transformer</node> is the first transduction model relying entirely on <node id=\"9\">self-attention</node> to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].",
        "model_architecture": "Most competitive neural <node id=\"3\">sequence transduction models</node> have an <node id=\"6\">encoder</node>-<node id=\"7\">decoder</node> structure [5, 2, 35]. Here, the <node id=\"6\">encoder</node> maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations z = (z_1, ..., z_n). Given z, the <node id=\"7\">decoder</node> then generates an output sequence (y_1, ..., y_m) of symbols one element at a time. At each step the model is <node id=\"23\">auto-regressive</node> [10], consuming the previously generated symbols as additional input when generating the next.\n\n### 3.1 Encoder and Decoder Stacks\n\n**<node id=\"6\">Encoder</node>:** The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a <node id=\"8\">multi-head self-attention</node> mechanism, and the second is a simple, position-wise fully connected <node id=\"12\">feed-forward network</node>. We employ a residual connection [11] around each of the two sub-layers, followed by <node id=\"13\">layer normalization</node> [1]. That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the <node id=\"14\">embedding</node> layers, produce outputs of dimension d_{model} = 512.\n\n**<node id=\"7\">Decoder</node>:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs <node id=\"8\">multi-head attention</node> over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the <node id=\"9\">self-attention</node> sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n\n### 3.2 Attention\n\nAn <node id=\"4\">attention function</node> can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n#### 3.2.1 Scaled Dot-Product Attention\n\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the <node id=\"32\">dot products</node> of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a <node id=\"34\">softmax function</node> to obtain the weights on the values.\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n\n```math\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\biggl(\\frac{QK^T}{\\sqrt{d_k}}\\biggr) V \\tag{1}\n```\n\n#### 3.2.2 Multi-Head Attention\n\nInstead of performing a single attention function with d_{model}-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys, and values h times with different learned projections to d_k, d_k, and d_v dimensions, respectively. On each of these projected versions, we perform the attention function in parallel, yielding d_v-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n\n```math\n\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head}_1, \\dots, \\mathrm{head}_h) W^O\n```\n\nwhere\n\n```math\n\\mathrm{head}_i = \\mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n```\n\nwith $W_i^Q \\in \\mathbb{R}^{d_{model}\\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model}\\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{model}\\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$.\n\nIn this work, we employ h = 8 heads, with d_k = d_v = d_{model}/h = 64.\n\n#### 3.2.3 Applications of Attention in our Model\n\nThe <node id=\"10\">Transformer</node> uses <node id=\"8\">multi-head attention</node> in three ways:\n\n* **Encoder–Decoder Attention:** queries come from the previous decoder layer, keys and values come from the encoder output.\n* **Encoder Self-Attention:** queries, keys, and values all come from the previous encoder layer.\n* **Decoder Self-Attention (masked):** same-layer queries, keys, and values with masking to prevent attending to future positions.\n\n### 3.3 Position-wise Feed-Forward Networks\n\nIn addition to attention sub-layers, each layer in our <node id=\"6\">encoder</node> and <node id=\"7\">decoder</node> contains a fully connected <node id=\"12\">feed-forward network</node> applied to each position separately:\n\n```math\n\\mathrm{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2 \\tag{2}\n```\n\nwith input/output dimension d_{model} = 512 and inner-layer dimension d_{ff} = 2048.\n\n### 3.4 Embeddings and Softmax\n\nWe use learned <node id=\"14\">embeddings</node> to convert tokens to vectors of dimension d_{model} and share the same weight matrix between the input embedding, output embedding, and pre-softmax linear transformation. <node id=\"14\">Embeddings</node> are scaled by $\\sqrt{d_{model}}$.\n\n### 3.5 Positional Encoding\n\nBecause the model has no recurrence or convolution, we inject positional information via sinusoidal encodings added to the <node id=\"14\">embeddings</node>:\n\n```math\nPE_{(pos, 2i)} = \\sin\\bigl(pos / 10000^{2i/d_{model}}\\bigr)\\\\\nPE_{(pos, 2i+1)} = \\cos\\bigl(pos / 10000^{2i/d_{model}}\\bigr)\n```",
        "why_self_attention": "We compare <node id=\"9\">self-attention</node>, recurrent, and convolutional layers by:\n\n1. Computational complexity per layer\n2. Parallelizability (minimum sequential operations)\n3. Maximum path length for long-range dependencies\n\n<node id=\"9\">Self-attention</node> has O(n^2 · d) complexity, O(1) sequential operations, and O(1) maximum path length.",
        "training": "### 5.1 Training Data and Batching\n\nWe trained on WMT 2014 English–German (4.5M sentence pairs, shared BPE vocab ~37K) and WMT 2014 English–French (36M sentences, vocab 32K) datasets. Batches contain ~25K source and target tokens.\n\n### 5.2 Hardware and Schedule\n\nBase models: 100K steps (~12h) on 8 P100 <node id=\"26\">GPUs</node>. Big models: 300K steps (~3.5 days).\n\n### 5.3 Optimizer\n\nWe used Adam [20] (β_1=0.9, β_2=0.98, ε=1e-9) with learning rate:\n\n```math\nlrate = d_{model}^{-0.5} \\cdot \\min(step^{-0.5}, step \\cdot warmup^{-1.5}) \n```\n\nwith warmup = 4000 steps.\n\n### 5.4 Regularization\n\n- Residual Dropout (P_{drop} = 0.1 base, 0.3 big)\n- Label Smoothing (ε_{ls} = 0.1)\n- Checkpoint Averaging (last 5 for base, 20 for big)",
        "results": "### 6.1 Machine Translation\n\n<node id=\"10\">Transformer</node> (big) achieves 28.4 BLEU on EN→DE and 41.8 BLEU on EN→FR, outperforming prior models at lower training cost (Table 2).\n\n### 6.2 Model Variations\n\nAblation studies on heads, dims, model size, dropout, and positional encodings (Table 3).\n\n### 6.3 English Constituency Parsing\n\n4-layer <node id=\"10\">Transformer</node> (d_{model}=1024) on WSJ: 91.3 F1 WSJ-only, 92.7 F1 semi-supervised, surpassing prior single-task parsers (Table 4).",
        "conclusion": "In this work, we presented the <node id=\"10\">Transformer</node>, the first <node id=\"3\">sequence transduction model</node> based entirely on <node id=\"4\">attention</node>, replacing recurrent and convolutional layers with <node id=\"8\">multi-headed self-attention</node>. It parallelizes efficiently, trains faster, and sets new state-of-the-art on translation and parsing. Future work includes other modalities, local/restricted attention, and reducing generation sequentiality. The code is available at https://github.com/tensorflow/tensor2tensor.",
        "acknowledgements": "We thank Nal Kalchbrenner and Stephan Gouws for comments and inspiration.",
        "original_paper_url": "https://arxiv.org/abs/1706.03762"
      }
    },
    {
      "id": 1,
      "label": "Neural Networks",
      "type": "concept",
      "description": "Computational models inspired by biological neural networks, consisting of interconnected nodes that process information.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 2,
      "label": "Backpropagation",
      "type": "concept",
      "description": "Algorithm for training neural networks by calculating gradients and updating weights through reverse differentiation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 3,
      "label": "Sequence Transduction Model",
      "type": "concept",
      "description": "Neural architectures that transform input sequences into output sequences, the core problem addressed by the Transformer.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 4,
      "label": "Attention Mechanisms",
      "type": "concept",
      "description": "Techniques that allow models to focus on relevant parts of input when making predictions.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 5,
      "label": "Recurrent Neural Networks",
      "type": "concept",
      "description": "Neural networks with loops that allow information to persist, designed for sequential data processing.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 6,
      "label": "Encoder",
      "type": "concept",
      "description": "Component that processes input sequence and creates a fixed-size representation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 7,
      "label": "Decoder",
      "type": "concept",
      "description": "Component that generates output sequence from the encoded representation.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 8,
      "label": "Multi-Head Attention",
      "type": "concept",
      "description": "Extension of attention mechanism that allows the model to jointly attend to information from different representation subspaces.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 9,
      "label": "Self Attention",
      "type": "concept",
      "description": "Attention mechanism where queries, keys, and values all come from the same sequence.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 10,
      "label": "Transformer",
      "type": "concept",
      "description": "Neural network architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 11,
      "label": "Positional Encoding",
      "type": "concept",
      "description": "Method to inject information about the relative or absolute position of tokens in a sequence.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 12,
      "label": "Feed-forward Network",
      "type": "concept",
      "description": "Position-wise fully connected layers that process each position separately and identically.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 13,
      "label": "Layer Normalization",
      "type": "concept",
      "description": "Normalization technique applied within each layer to stabilize training.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 14,
      "label": "Embedding",
      "type": "concept",
      "description": "Dense vector representations of discrete tokens like words or subwords.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 15,
      "label": "Convolutional Neural Networks",
      "type": "concept",
      "description": "Neural networks specialized for processing grid-like data, using convolution operations.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 16,
      "label": "Convolution",
      "type": "concept",
      "description": "Mathematical operation that applies filters to local regions of input data.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 17,
      "label": "Long Short Term Memory",
      "type": "concept",
      "description": "Type of RNN designed to avoid vanishing gradient problems through gating mechanisms.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 18,
      "label": "Gated Recurrent Neural Network",
      "type": "concept",
      "description": "RNN variant that uses gates to control information flow, simpler than LSTM.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 19,
      "label": "Sequence Modeling",
      "type": "concept",
      "description": "Techniques for processing and predicting sequential data like text or time series.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 20,
      "label": "Machine Translation",
      "type": "concept",
      "description": "Automated translation of text from one language to another using computational methods.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 21,
      "label": "Language Modeling",
      "type": "concept",
      "description": "Statistical modeling of natural language to predict word sequences and text patterns.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 22,
      "label": "Hidden States",
      "type": "concept",
      "description": "Internal representations that capture information processed at each step in sequence models.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 23,
      "label": "Auto-Regressive",
      "type": "concept",
      "description": "Generation method where each output depends on previously generated outputs.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 24,
      "label": "Parallelization",
      "type": "concept",
      "description": "Ability to process multiple computations simultaneously, key advantage of Transformers.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 25,
      "label": "Training a Model",
      "type": "concept",
      "description": "Process of optimizing neural network parameters using data and loss functions.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 26,
      "label": "GPU",
      "type": "concept",
      "description": "Graphics Processing Unit optimized for parallel computations, essential for training large models.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 27,
      "label": "Batching",
      "type": "concept",
      "description": "Processing multiple examples simultaneously to improve computational efficiency.",
      "difficulty": 2,
      "domain": "tech"
    },
    {
      "id": 28,
      "label": "Dependencies",
      "type": "concept",
      "description": "Relationships between elements in sequences that models need to capture.",
      "difficulty": 3,
      "domain": "tech"
    },
    {
      "id": 29,
      "label": "ConvS2S",
      "type": "concept",
      "description": "Convolutional sequence-to-sequence model, predecessor to Transformer architecture.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 30,
      "label": "ByteNet",
      "type": "concept",
      "description": "Convolutional neural network for sequence modeling with dilated convolutions.",
      "difficulty": 4,
      "domain": "tech"
    },
    {
      "id": 31,
      "label": "Matrix Multiplication",
      "type": "concept",
      "description": "Mathematical operation for combining two matrices, fundamental to neural network computations.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 32,
      "label": "Dot Product",
      "type": "concept",
      "description": "Mathematical operation that multiplies corresponding elements of vectors and sums them, used in attention scoring.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 33,
      "label": "Vector Operations",
      "type": "concept",
      "description": "Basic mathematical operations on vectors like addition, subtraction, and scalar multiplication.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 34,
      "label": "Softmax Function",
      "type": "concept",
      "description": "Activation function that converts a vector of real numbers into a probability distribution.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 35,
      "label": "Gradient",
      "type": "concept",
      "description": "Vector of partial derivatives indicating the direction and rate of steepest increase of a function.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 36,
      "label": "Derivative",
      "type": "concept",
      "description": "Mathematical concept measuring how a function changes as its input changes.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 37,
      "label": "Chain Rule",
      "type": "concept",
      "description": "Mathematical rule for computing the derivative of composite functions, essential for backpropagation.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 38,
      "label": "Partial Derivatives",
      "type": "concept",
      "description": "Derivatives of functions with multiple variables, taken with respect to one variable while holding others constant.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 39,
      "label": "Trigonometric Functions",
      "type": "concept",
      "description": "Mathematical functions like sine and cosine used in positional encoding for sequence position.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 40,
      "label": "Mean",
      "type": "concept",
      "description": "Average value of a set of numbers, used in normalization techniques.",
      "difficulty": 1,
      "domain": "math"
    },
    {
      "id": 41,
      "label": "Variance",
      "type": "concept",
      "description": "Measure of how spread out numbers are from their mean, used in layer normalization.",
      "difficulty": 2,
      "domain": "math"
    },
    {
      "id": 42,
      "label": "Linear Transformation",
      "type": "concept",
      "description": "Mathematical mapping between vector spaces that preserves vector addition and scalar multiplication.",
      "difficulty": 3,
      "domain": "math"
    },
    {
      "id": 43,
      "label": "Weighted Sum",
      "type": "concept",
      "description": "Sum of values each multiplied by a corresponding weight, fundamental to neural network operations.",
      "difficulty": 2,
      "domain": "math"
    }
  ],
  "links": [
    {
      "source": 1,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 2,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 10,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 8,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 11,
      "target": 0,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 43,
      "target": 1,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 1,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 35,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 37,
      "target": 2,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 3,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 32,
      "target": 4,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 34,
      "target": 4,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 5,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 6,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 7,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 8,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 42,
      "target": 8,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 4,
      "target": 9,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 6,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 7,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 8,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 9,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 11,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 12,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 13,
      "target": 10,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 39,
      "target": 11,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 11,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 43,
      "target": 12,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 42,
      "target": 12,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 40,
      "target": 13,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 41,
      "target": 13,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 14,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 16,
      "target": 15,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 16,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 17,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 18,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 1,
      "target": 19,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 3,
      "target": 20,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 21,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 5,
      "target": 22,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 7,
      "target": 23,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 26,
      "target": 24,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 2,
      "target": 25,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 27,
      "target": 25,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 19,
      "target": 28,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 15,
      "target": 29,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 3,
      "target": 29,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 15,
      "target": 30,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 31,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 32,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 35,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 38,
      "target": 35,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 37,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 36,
      "target": 38,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 40,
      "target": 41,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 31,
      "target": 42,
      "relation": "prerequisite",
      "value": 1
    },
    {
      "source": 33,
      "target": 43,
      "relation": "prerequisite",
      "value": 1
    }
  ]
} 